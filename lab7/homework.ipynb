{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94121217",
   "metadata": {},
   "source": [
    "# Evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9108a7",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350287bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acce6db19b2946dd89b7c092e1e3d590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b177d7386640f680aec2e6c53c0115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a459c00e7524e61846eefd6e31ede23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb512028c0584c6bbed25577cf261245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88a55945cdf4de0b267db2e117da00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814223ed153f4e5a92288a2e46ef7779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "transformer = AutoModel.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"sentence-transformers/multi-qa-mpnet-base-cos-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e215cf",
   "metadata": {},
   "source": [
    "## Create a sample input text and tokenize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff53ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Pope John Paul II[b] (born Karol Józef Wojtyła;[c] 18 May 1920 – 2 April 2005) was head of the Catholic Church and sovereign of the Vatican City from 16 October 1978 until his death in 2005. He was the first non-Italian pope since Adrian VI in the 16th century, as well as the third-longest-serving pope in history, after Pius IX and St. Peter.[d]\n",
    "\n",
    "In his youth, Wojtyła dabbled in stage acting. He graduated with excellent grades from an all-boys high school in Wadowice, Poland, in 1938, soon after which World War II broke out. During the war, to avoid being kidnapped and sent to a German forced labour camp, he signed up for work in harsh conditions in a quarry. Wojtyła eventually took up acting and developed a love for the profession and participated at a local theatre. The linguistically skilled Wojtyła wanted to study Polish at university. Encouraged by a conversation with Adam Stefan Sapieha, he decided to study theology and become a priest. Eventually, Wojtyła rose to the position of Archbishop of Kraków and then a cardinal, both positions held by his mentor. Wojtyła was elected pope on the third day of the October 1978 conclave, becoming one of the youngest popes in history. The conclave was called after the death of John Paul I, who served only 33 days as pope. Wojtyła adopted the name of his predecessor in tribute to him.\n",
    "\"\"\"\n",
    "tokenized_text = tokenizer(\n",
    "    text=text, padding=True, truncation=True, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e85018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,  4835,  2202,  2707,  2466,  1035,  1042,  1037,  1010,  2145,\n",
       "         10560, 13157, 23262, 24189,  3505,  3727, 22976,  1029,  1035,  1043,\n",
       "          1037,  2328,  2093,  4448,  1520,  1020,  2262,  2388,  1011,  2005,\n",
       "          2136,  2001,  2000,  3238,  2281,  2002, 11078,  2001,  2000, 12115,\n",
       "          2107,  2017,  2389,  2259,  3305,  2131,  2014,  2335,  2003,  2388,\n",
       "          1016,  2006,  2005,  2000,  2038,  2516,  1015,  3063,  4835,  2148,\n",
       "          7922,  6823,  2003,  2000,  5771,  2305,  1014,  2008,  2096,  2008,\n",
       "          2000,  2357,  1015,  6497,  1015,  3533,  4835,  2003,  2385,  1014,\n",
       "          2048, 14367, 11818,  2002,  2362,  1016,  2852,  1016,  1035,  1044,\n",
       "          1037,  2003,  2014,  3364,  1014, 24189,  3505,  3727, 22976,  4834,\n",
       "         12824,  2003,  2758,  3776,  1016,  2006,  3856,  2011,  6585,  7026,\n",
       "          2017,  2023,  2039,  1015,  3341,  2156,  2086,  2003, 11337,  3531,\n",
       "         23429,  1014,  3739,  1014,  2003,  4264,  1014,  2578,  2048,  2033,\n",
       "          2092,  2166,  2466,  3635,  2045,  1016,  2080,  2000,  2166,  1014,\n",
       "          2004,  4472,  2112, 11368,  2002,  2745,  2004,  1041,  2450,  3144,\n",
       "          4432,  3413,  1014,  2006,  2776,  2043,  2009,  2151,  2003,  8405,\n",
       "          3789,  2003,  1041, 12911,  1016, 24189,  3505,  3727, 22976,  2780,\n",
       "          2169,  2043,  3776,  2002,  2768,  1041,  2297,  2009,  2000,  9522,\n",
       "          2002,  4198,  2016,  1041,  2338,  3008,  1016,  2000, 12162,  3977,\n",
       "         10575, 24189,  3505,  3727, 22976,  2363,  2004,  2821,  3911,  2016,\n",
       "          2122,  1016,  6632,  2015,  1041,  4516,  2011,  4209,  8856, 20070,\n",
       "          2670,  3274,  1014,  2006,  2791,  2004,  2821,  8010,  2002,  2472,\n",
       "          1041,  5015,  1016,  2780,  1014, 24189,  3505,  3727, 22976,  3127,\n",
       "          2004,  2000,  2601,  2001,  6511,  2001, 14580,  2002,  2063,  1041,\n",
       "          7189,  1014,  2123,  4464,  2222,  2015,  2014, 10783,  1016, 24189,\n",
       "          3505,  3727, 22976,  2005,  2704,  4835,  2010,  2000,  2357,  2158,\n",
       "          2001,  2000,  2259,  3305,  9534, 23654,  1014,  3356,  2032,  2001,\n",
       "          2000,  6591,  4835,  2019,  2003,  2385,  1016,  2000,  9534, 23654,\n",
       "          2005,  2174,  2048,  2000,  2335,  2001,  2202,  2707,  1049,  1014,\n",
       "          2044,  2370,  2073,  3947,  2424,  2008,  4835,  1016, 24189,  3505,\n",
       "          3727, 22976,  4237,  2000,  2175,  2001,  2014,  8650,  2003,  7054,\n",
       "          2004,  2036,  1016,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60634ef8",
   "metadata": {},
   "source": [
    "## Measure the inference time of the model in various inference modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90170f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INFERENCE TIME COMPARISON (average over 100 runs)\n",
      "============================================================\n",
      "1. No optimizations:           884.1026 ms\n",
      "2. transformer.eval():               889.8580 ms\n",
      "3. transformer.eval() + no_grad():   868.4453 ms\n",
      "4. transformer.eval() + inference:   866.7855 ms\n",
      "============================================================\n",
      "Speedup (no_grad vs baseline):      1.02x\n",
      "Speedup (inference vs baseline):    1.02x\n",
      "Speedup (inference vs no_grad):     1.00x\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "transformer.to(device)\n",
    "\n",
    "num_runs = 100\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = transformer(**tokenized_text)\n",
    "time_no_opt = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = transformer(**tokenized_text)\n",
    "time_eval = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    with torch.no_grad():\n",
    "        outputs = transformer(**tokenized_text)\n",
    "time_eval_no_grad = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    with torch.inference_mode():\n",
    "        outputs = transformer(**tokenized_text)\n",
    "time_eval_inference = (time.time() - start) / num_runs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFERENCE TIME COMPARISON (average over 100 runs)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"1. No optimizations:           {time_no_opt * 1000:.4f} ms\")\n",
    "print(f\"2. transformer.eval():               {time_eval * 1000:.4f} ms\")\n",
    "print(f\"3. transformer.eval() + no_grad():   {time_eval_no_grad * 1000:.4f} ms\")\n",
    "print(f\"4. transformer.eval() + inference:   {time_eval_inference * 1000:.4f} ms\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Speedup (no_grad vs baseline):      {time_no_opt / time_eval_no_grad:.2f}x\")\n",
    "print(f\"Speedup (inference vs baseline):    {time_no_opt / time_eval_inference:.2f}x\")\n",
    "print(\n",
    "    f\"Speedup (inference vs no_grad):     {time_eval_no_grad / time_eval_inference:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bf76a",
   "metadata": {},
   "source": [
    "# PyTorch model compilation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99715c60",
   "metadata": {},
   "source": [
    "## Compile the model using `torch.compile()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e3b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "transformer.eval()\n",
    "transformer.compile()\n",
    "transformer(**tokenized_text)\n",
    "compilation_plus_warm_up_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79040add",
   "metadata": {},
   "source": [
    "## Measure the inference time, Calculate the speedup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e703e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPILED INFERENCE TIME COMPARISON (average over 100 runs)\n",
      "============================================================\n",
      "Compilation + warm_up:           41.34 s\n",
      "============================================================\n",
      "Speedup (compiled vs eval):      0.80x\n",
      "Speedup (compiled vs no_grad):      0.78x\n",
      "Speedup (compiled vs inference):    0.78x\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = transformer(**tokenized_text)\n",
    "time_compiled = (time.time() - start) / num_runs\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPILED INFERENCE TIME COMPARISON (average over 100 runs)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Compilation + warm_up:           {compilation_plus_warm_up_time:.2f} s\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Speedup (compiled vs eval):      {time_eval / time_compiled:.2f}x\")\n",
    "print(f\"Speedup (compiled vs no_grad):      {time_eval_no_grad / time_compiled:.2f}x\")\n",
    "print(f\"Speedup (compiled vs inference):    {time_eval_inference / time_compiled:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44988bfa",
   "metadata": {},
   "source": [
    "# Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e8531",
   "metadata": {},
   "source": [
    "## Quantize the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c141a098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3441829427.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  model_quantized = torch.ao.quantization.quantize_dynamic(\n"
     ]
    }
   ],
   "source": [
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "model_quantized = torch.ao.quantization.quantize_dynamic(\n",
    "    model=transformer, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bff8e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNetModel(\n",
      "  (embeddings): MPNetEmbeddings(\n",
      "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): MPNetEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x MPNetLayer(\n",
      "        (attention): MPNetAttention(\n",
      "          (attn): MPNetSelfAttention(\n",
      "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (intermediate): MPNetIntermediate(\n",
      "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): MPNetOutput(\n",
      "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (relative_attention_bias): Embedding(32, 12)\n",
      "  )\n",
      "  (pooler): MPNetPooler(\n",
      "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977376f0",
   "metadata": {},
   "source": [
    "## Save both models to disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0fe27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "models_directory = Path(\"models\")\n",
    "models_directory.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(transformer.state_dict(), f=f\"{models_directory}/orig_model.pt\")\n",
    "torch.save(model_quantized.state_dict(), f=f\"{models_directory}/quantized_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517ba81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the original model: 417.73 MB\n",
      "Size of quantized model: 173.10 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "orig_model_size = os.path.getsize(f\"{models_directory}/orig_model.pt\")\n",
    "quantized_model_size = os.path.getsize(f\"{models_directory}/quantized_model.pt\")\n",
    "\n",
    "print(f\"Size of the original model: {orig_model_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Size of quantized model: {quantized_model_size / 1024 / 1024:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed32e1",
   "metadata": {},
   "source": [
    "## Compare the inference speed and speedup on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6049cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model_quantized.eval()\n",
    "model_quantized.compile()\n",
    "model_quantized(**tokenized_text)\n",
    "quantized_compilation_plus_warm_up_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ORIGINAL MODEL COMPILED INFERENCE TIME COMPARISON\n",
      "============================================================\n",
      "Compilation + warm_up:           41.34 s\n",
      "============================================================\n",
      "Speedup (compiled vs eval):      0.80x\n",
      "Speedup (compiled vs no_grad):      0.78x\n",
      "Speedup (compiled vs inference):    0.78x\n",
      "\n",
      "============================================================\n",
      "QUANTIZED MODEL COMPILED INFERENCE TIME COMPARISON\n",
      "============================================================\n",
      "Compilation + warm_up:           41.34 s\n",
      "============================================================\n",
      "Speedup (compiled vs eval):      0.38x\n",
      "Speedup (compiled vs no_grad):      0.37x\n",
      "Speedup (compiled vs inference):    0.37x\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = model_quantized(**tokenized_text)\n",
    "quantized_time_compiled = (time.time() - start) / num_runs\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ORIGINAL MODEL COMPILED INFERENCE TIME COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Compilation + warm_up:           {compilation_plus_warm_up_time:.2f} s\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Speedup (compiled vs eval):      {time_eval / time_compiled:.2f}x\")\n",
    "print(f\"Speedup (compiled vs no_grad):      {time_eval_no_grad / time_compiled:.2f}x\")\n",
    "print(\n",
    "    f\"Speedup (compiled vs inference):    {time_eval_inference / time_compiled:.2f}x\\n\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUANTIZED MODEL COMPILED INFERENCE TIME COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Compilation + warm_up:           {compilation_plus_warm_up_time:.2f} s\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Speedup (compiled vs eval):      {time_eval / quantized_time_compiled:.2f}x\")\n",
    "print(\n",
    "    f\"Speedup (compiled vs no_grad):      {time_eval_no_grad / quantized_time_compiled:.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"Speedup (compiled vs inference):    {time_eval_inference / quantized_time_compiled:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "834ae0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = model_quantized(**tokenized_text)\n",
    "quantized_time_no_opt = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    outputs = model_quantized(**tokenized_text)\n",
    "quantized_time_eval = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model_quantized(**tokenized_text)\n",
    "quantized_time_eval_no_grad = (time.time() - start) / num_runs\n",
    "\n",
    "transformer.eval()\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    with torch.inference_mode():\n",
    "        outputs = model_quantized(**tokenized_text)\n",
    "quantized_time_eval_inference = (time.time() - start) / num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54589d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEEDUP: BASE MODEL vs QUANTIZED MODEL\n",
      "============================================================\n",
      "1. No optimizations:           0.38x\n",
      "2. eval():                     0.38x\n",
      "3. eval() + no_grad():         0.36x\n",
      "4. eval() + inference_mode():  0.37x\n",
      "\n",
      "============================================================\n",
      "SPEEDUP: BASE MODEL vs QUANTIZED + COMPILED MODEL\n",
      "============================================================\n",
      "Compilation + warm_up time:    41.3387 s\n",
      "vs Base no opt:                0.38x\n",
      "vs Base eval():                0.38x\n",
      "vs Base no_grad():             0.37x\n",
      "vs Base inference_mode():      0.37x\n",
      "\n",
      "============================================================\n",
      "BEST CONFIGURATION COMPARISON\n",
      "============================================================\n",
      "Base (best):                   866.7855 ms\n",
      "Quantized (best):              2337.9440 ms\n",
      "Quantized + Compiled:          2340.1477 ms\n",
      "\n",
      "Quantization speedup:          0.37x\n",
      "Quantization + Compilation:    0.37x\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPEEDUP: BASE MODEL vs QUANTIZED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"1. No optimizations:           {time_no_opt / quantized_time_no_opt:.2f}x\")\n",
    "print(f\"2. eval():                     {time_eval / quantized_time_eval:.2f}x\")\n",
    "print(\n",
    "    f\"3. eval() + no_grad():         {time_eval_no_grad / quantized_time_eval_no_grad:.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"4. eval() + inference_mode():  {time_eval_inference / quantized_time_eval_inference:.2f}x\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SPEEDUP: BASE MODEL vs QUANTIZED + COMPILED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Compilation + warm_up time:    {compilation_plus_warm_up_time:.4f} s\")\n",
    "print(f\"vs Base no opt:                {time_no_opt / quantized_time_compiled:.2f}x\")\n",
    "print(f\"vs Base eval():                {time_eval / quantized_time_compiled:.2f}x\")\n",
    "print(\n",
    "    f\"vs Base no_grad():             {time_eval_no_grad / quantized_time_compiled:.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"vs Base inference_mode():      {time_eval_inference / quantized_time_compiled:.2f}x\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST CONFIGURATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Base (best):                   {time_eval_inference * 1000:.4f} ms\")\n",
    "print(f\"Quantized (best):              {quantized_time_eval_inference * 1000:.4f} ms\")\n",
    "print(f\"Quantized + Compiled:          {quantized_time_compiled * 1000:.4f} ms\")\n",
    "print(\n",
    "    f\"\\nQuantization speedup:          {time_eval_inference / quantized_time_eval_inference:.2f}x\"\n",
    ")\n",
    "print(\n",
    "    f\"Quantization + Compilation:    {time_eval_inference / quantized_time_compiled:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4e937",
   "metadata": {},
   "source": [
    "Quantization is not beneficial for CPU inference on this model size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960347a",
   "metadata": {},
   "source": [
    "# GPU optimization strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e630b92",
   "metadata": {},
   "source": [
    "## Compare inference time of:\n",
    "\n",
    "- torch.compile() with default settings\n",
    "- torch.compile() with mode=\"max-autotune\"\n",
    "- torch.compile() with mode=\"max-autotune-no-cudagraphs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text_100 = text[:100]\n",
    "text_500 = text[:500]\n",
    "\n",
    "text_sizes = {\"100 chars\": text_100, \"500 chars\": text_500, \"1350 chars\": text}\n",
    "\n",
    "\n",
    "def compare_models(text: str, text_size: str):\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"COMPARING MODELS INPUT TEXT SIZE OF: {text_size}\")\n",
    "    print(\"=\" * 60)\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.pin_memory() for k, v in inputs.items()}\n",
    "\n",
    "    model_compiled_default = torch.compile(transformer)\n",
    "    model_compiled_default.eval()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(10):\n",
    "            _ = model_compiled_default(**inputs)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model_compiled_default(**inputs)\n",
    "    time_compiled_default = (time.time() - start) / num_runs\n",
    "\n",
    "    model_compiled_max_autotune = torch.compile(transformer, mode=\"max-autotune\")\n",
    "    model_compiled_max_autotune.eval()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(10):\n",
    "            _ = model_compiled_max_autotune(**inputs)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model_compiled_max_autotune(**inputs)\n",
    "    time_compiled_max_autotune = (time.time() - start) / num_runs\n",
    "\n",
    "    model_compiled_no_cudagraphs = torch.compile(\n",
    "        transformer, mode=\"max-autotune-no-cudagraphs\"\n",
    "    )\n",
    "    model_compiled_no_cudagraphs.eval()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(10):\n",
    "            _ = model_compiled_no_cudagraphs(**inputs)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(num_runs):\n",
    "            _ = model_compiled_no_cudagraphs(**inputs)\n",
    "    time_compiled_no_cudagraphs = (time.time() - start) / num_runs\n",
    "\n",
    "    print(\"\\nTORCH.COMPILE() MODE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Default mode:                  {time_compiled_default * 1000:.4f} ms\")\n",
    "    print(f\"max-autotune:                  {time_compiled_max_autotune * 1000:.4f} ms\")\n",
    "    print(f\"max-autotune-no-cudagraphs:    {time_compiled_no_cudagraphs * 1000:.4f} ms\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\n",
    "        f\"Speedup (max-autotune vs default):        {time_compiled_default / time_compiled_max_autotune:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup (no-cudagraphs vs default):       {time_compiled_default / time_compiled_no_cudagraphs:.2f}x\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Speedup (max-autotune vs no-cudagraphs):  {time_compiled_no_cudagraphs / time_compiled_max_autotune:.2f}x\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9510b4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARING MODELS INPUT TEXT SIZE OF: 100 chars\n",
      "============================================================\n",
      "\n",
      "TORCH.COMPILE() MODE COMPARISON\n",
      "============================================================\n",
      "Default mode:                  129.1312 ms\n",
      "max-autotune:                  130.1898 ms\n",
      "max-autotune-no-cudagraphs:    137.9546 ms\n",
      "============================================================\n",
      "Speedup (max-autotune vs default):        0.99x\n",
      "Speedup (no-cudagraphs vs default):       0.94x\n",
      "Speedup (max-autotune vs no-cudagraphs):  1.06x\n",
      "============================================================\n",
      "COMPARING MODELS INPUT TEXT SIZE OF: 500 chars\n",
      "============================================================\n",
      "\n",
      "TORCH.COMPILE() MODE COMPARISON\n",
      "============================================================\n",
      "Default mode:                  406.4013 ms\n",
      "max-autotune:                  406.9343 ms\n",
      "max-autotune-no-cudagraphs:    416.1094 ms\n",
      "============================================================\n",
      "Speedup (max-autotune vs default):        1.00x\n",
      "Speedup (no-cudagraphs vs default):       0.98x\n",
      "Speedup (max-autotune vs no-cudagraphs):  1.02x\n",
      "============================================================\n",
      "COMPARING MODELS INPUT TEXT SIZE OF: 1350 chars\n",
      "============================================================\n",
      "\n",
      "TORCH.COMPILE() MODE COMPARISON\n",
      "============================================================\n",
      "Default mode:                  1089.6288 ms\n",
      "max-autotune:                  1099.9176 ms\n",
      "max-autotune-no-cudagraphs:    1096.8534 ms\n",
      "============================================================\n",
      "Speedup (max-autotune vs default):        0.99x\n",
      "Speedup (no-cudagraphs vs default):       0.99x\n",
      "Speedup (max-autotune vs no-cudagraphs):  1.00x\n"
     ]
    }
   ],
   "source": [
    "for text_name, text_chunk in text_sizes.items():\n",
    "    compare_models(text_chunk, text_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cabeb17",
   "metadata": {},
   "source": [
    "The advanced compilation modes provide no meaningful performance improvement - they're essentially the same as default mode or slightly slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cf7aa",
   "metadata": {},
   "source": [
    "# Changing numerical precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedaef4e",
   "metadata": {},
   "source": [
    "## Check if your GPU supports Tensor Cores (capability >= (7,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a9c884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 5)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d12879",
   "metadata": {},
   "source": [
    "## Measure inference time with:\n",
    "\n",
    "- full precision (float32)\n",
    "- manual half-precision (float16)\n",
    "- automatic mixed precision (torch.autocast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11e7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "\n",
      "Testing full precision (float32)...\n",
      "  Time: 23.0370 ms\n",
      "\n",
      "Testing manual half-precision (float16)...\n",
      "  Time: 12.3941 ms\n",
      "\n",
      "Testing automatic mixed precision (autocast)...\n",
      "  Time: 12.0268 ms\n",
      "\n",
      "============================================================\n",
      "PRECISION COMPARISON (average over 100 runs)\n",
      "============================================================\n",
      "Full precision (float32):      23.0370 ms\n",
      "Manual half (float16):         12.3941 ms\n",
      "Automatic mixed (autocast):    12.0268 ms\n",
      "============================================================\n",
      "Speedup (float16 vs float32):    1.86x\n",
      "Speedup (autocast vs float32):   1.92x\n",
      "Speedup (float16 vs autocast):   0.97x\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "print(\"\\nTesting full precision (float32)...\")\n",
    "transformer_fp32 = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")  \n",
    "transformer_fp32 = transformer_fp32.to(device).float()\n",
    "transformer_fp32.eval()\n",
    "\n",
    "tokenized_text_fp32 = {k: v.to(device) for k, v in tokenized_text.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        _ = transformer_fp32(**tokenized_text_fp32)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = transformer_fp32(**tokenized_text_fp32)\n",
    "    time_float32 = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"  Time: {time_float32*1000:.4f} ms\")\n",
    "\n",
    "print(\"\\nTesting manual half-precision (float16)...\")\n",
    "transformer_fp16 = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")  \n",
    "transformer_fp16 = transformer_fp16.to(device).half()\n",
    "transformer_fp16.eval()\n",
    "\n",
    "tokenized_text_fp16 = {k: v.to(device) for k, v in tokenized_text.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        _ = transformer_fp16(**tokenized_text_fp16)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = transformer_fp16(**tokenized_text_fp16)\n",
    "    time_float16 = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"  Time: {time_float16*1000:.4f} ms\")\n",
    "\n",
    "print(\"\\nTesting automatic mixed precision (autocast)...\")\n",
    "transformer_autocast = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "transformer_autocast = transformer_autocast.to(device).float()\n",
    "transformer_autocast.eval()\n",
    "\n",
    "tokenized_text_autocast = {k: v.to(device) for k, v in tokenized_text.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            _ = transformer_autocast(**tokenized_text_autocast)\n",
    "    \n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            _ = transformer_autocast(**tokenized_text_autocast)\n",
    "    time_autocast = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"  Time: {time_autocast*1000:.4f} ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRECISION COMPARISON (average over 100 runs)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Full precision (float32):      {time_float32*1000:.4f} ms\")\n",
    "print(f\"Manual half (float16):         {time_float16*1000:.4f} ms\")\n",
    "print(f\"Automatic mixed (autocast):    {time_autocast*1000:.4f} ms\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Speedup (float16 vs float32):    {time_float32/time_float16:.2f}x\")\n",
    "print(f\"Speedup (autocast vs float32):   {time_float32/time_autocast:.2f}x\")\n",
    "print(f\"Speedup (float16 vs autocast):   {time_autocast/time_float16:.2f}x\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189a0ae",
   "metadata": {},
   "source": [
    "In practice, I would use autocast, because it achieved best performance while being the easiest to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3caa99",
   "metadata": {},
   "source": [
    "# ONNX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a793f",
   "metadata": {},
   "source": [
    "## Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes on CPU and measure inference time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87a3c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
      "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx\n",
      "Successfully installed onnx-1.19.1\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c565a245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3496328736.py:16: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    }
   ],
   "source": [
    "model_cpu = transformer = (\n",
    "    AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "    .eval()\n",
    "    .cpu()\n",
    ")\n",
    "\n",
    "sample_text = \"This is a sample input text for ONNX export.\"\n",
    "sample_input = tokenizer(\n",
    "    sample_text,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "print(\"Exporting model to ONNX...\")\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamo=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a0fb322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting coloredlogs (from onnxruntime)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.23.2\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "864db7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ONNX RUNTIME OPTIMIZATION COMPARISON\n",
      "============================================================\n",
      "\n",
      "1. Testing ONLINE optimization...\n",
      "   Cold start time: 0.6458 s\n",
      "   Inference time:  57.6660 ms\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "ort_inputs = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"].numpy(),\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"].numpy(),\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ONNX RUNTIME OPTIMIZATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "print(\"\\n1. Testing ONLINE optimization...\")\n",
    "start_cold = time.time()\n",
    "sess_options_online = ort.SessionOptions()\n",
    "sess_options_online.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "ort_session_online = ort.InferenceSession(\n",
    "    \"model.onnx\", sess_options=sess_options_online, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "_ = ort_session_online.run(None, ort_inputs)\n",
    "cold_start_online = time.time() - start_cold\n",
    "\n",
    "for _ in range(10):\n",
    "    _ = ort_session_online.run(None, ort_inputs)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    _ = ort_session_online.run(None, ort_inputs)\n",
    "inference_time_online = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"   Cold start time: {cold_start_online:.4f} s\")\n",
    "print(f\"   Inference time:  {inference_time_online * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Testing OFFLINE optimization...\n",
      "   Creating offline optimized model...\n",
      "   Optimization time: 2.1047 s\n",
      "   Cold start time: 0.8375 s\n",
      "   Inference time:  57.8757 ms\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n2. Testing OFFLINE optimization...\")\n",
    "print(\"   Creating offline optimized model...\")\n",
    "start_optimize = time.time()\n",
    "sess_options_offline = ort.SessionOptions()\n",
    "sess_options_offline.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "sess_options_offline.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "_ = ort.InferenceSession(\"model.onnx\", sess_options_offline)\n",
    "optimization_time = time.time() - start_optimize\n",
    "print(f\"   Optimization time: {optimization_time:.4f} s\")\n",
    "\n",
    "start_cold = time.time()\n",
    "sess_options_load = ort.SessionOptions()\n",
    "sess_options_load.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "ort_session_offline = ort.InferenceSession(\n",
    "    \"model_optimized.onnx\", \n",
    "    sess_options=sess_options_load, \n",
    "    providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "_ = ort_session_offline.run(None, ort_inputs)\n",
    "cold_start_offline = time.time() - start_cold\n",
    "\n",
    "for _ in range(10):\n",
    "    _ = ort_session_offline.run(None, ort_inputs)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(num_runs):\n",
    "    _ = ort_session_offline.run(None, ort_inputs)\n",
    "inference_time_offline = (time.time() - start) / num_runs\n",
    "\n",
    "print(f\"   Cold start time: {cold_start_offline:.4f} s\")\n",
    "print(f\"   Inference time:  {inference_time_offline*1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5834492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Cold Start Time (session creation + first inference):\n",
      "   Online optimization:   0.6458 s\n",
      "   Offline optimization:  0.8375 s\n",
      "   Speedup:               0.77x\n",
      "\n",
      "Inference Time (average over 100 runs):\n",
      "   Online optimization:   57.6660 ms\n",
      "   Offline optimization:  57.8757 ms\n",
      "   Speedup:               1.00x\n"
     ]
    }
   ],
   "source": [
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCold Start Time (session creation + first inference):\")\n",
    "print(f\"   Online optimization:   {cold_start_online:.4f} s\")\n",
    "print(f\"   Offline optimization:  {cold_start_offline:.4f} s\")\n",
    "print(f\"   Speedup:               {cold_start_online / cold_start_offline:.2f}x\")\n",
    "\n",
    "print(\"\\nInference Time (average over 100 runs):\")\n",
    "print(f\"   Online optimization:   {inference_time_online * 1000:.4f} ms\")\n",
    "print(f\"   Offline optimization:  {inference_time_offline * 1000:.4f} ms\")\n",
    "print(\n",
    "    f\"   Speedup:               {inference_time_online / inference_time_offline:.2f}x\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dbcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
